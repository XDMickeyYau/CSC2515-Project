{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMoSawM0cQUX",
        "outputId": "16cc5c52-643d-48cd-863a-556d68fba7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.8/dist-packages (1.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=c787706acf4dd157da1882e723d8624514bbff0fc6007339424f562ac867367f\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn matplotlib pandas numpy autograd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict\n",
        "\n",
        "def load_cifar_100_data(file_dir):\n",
        "    train_data_dict = unpickle(os.path.join(file_dir, \"train\"))\n",
        "    test_data_dict = unpickle(os.path.join(file_dir, \"test\"))\n",
        "    x_train = train_data_dict[\"data\"].reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
        "    y_train = train_data_dict[\"coarse_labels\"]\n",
        "    x_test = test_data_dict[\"data\"].reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
        "    y_test = test_data_dict[\"coarse_labels\"]\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "def load_cifar_10_data(file_dir):\n",
        "    x_train = None\n",
        "    y_train = None\n",
        "    for i in range(1, 6):\n",
        "      train_data_batch_dict = unpickle(os.path.join(file_dir, \"data_batch_\"+str(i)))\n",
        "      x_train_batch = train_data_batch_dict[\"data\"].reshape(10000,3072)\n",
        "      y_train_batch = train_data_batch_dict[\"labels\"]\n",
        "      x_train = x_train_batch if x_train is None else np.vstack((x_train,x_train_batch))\n",
        "      y_train = y_train_batch if y_train is None else y_train + y_train_batch\n",
        "    x_train = x_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
        "    test_data_dict = unpickle(os.path.join(file_dir, \"test_batch\"))\n",
        "    x_test = test_data_dict[\"data\"].reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
        "    y_test = np.array(unpickle(os.path.join(\".\", \"test_batch\"))[\"labels\"])\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# flattern cifar data from N*32*32*3 to N*3072\n",
        "def flattern_data(x):\n",
        "    samples = x.shape[0]\n",
        "    flattern_shape = 1\n",
        "    for dim in x.shape[1:]:\n",
        "        flattern_shape *= dim\n",
        "    return x.reshape(samples, flattern_shape)\n",
        "\n",
        "# reconstruct data from N*3072 to N*32*32*3\n",
        "def construct_image_from_flattern(x, colored = True):\n",
        "    samples = x.shape[0]\n",
        "    if colored:\n",
        "      return x.reshape(samples, 32, 32, 3).astype(\"uint8\")\n",
        "    else:\n",
        "      return x.reshape(samples, 32, 32).astype(\"uint8\")\n",
        "\n",
        "#visualize image data, displayed on row*col grid, x's 1st-dim >= (row*col)\n",
        "def visualize_data(row, col, plt_size, x):\n",
        "    fig, axes1 = plt.subplots(row, col, figsize=(plt_size, plt_size))\n",
        "    i = 0\n",
        "    for j in range(row):\n",
        "        for k in range(col):\n",
        "            if i >= len(x):\n",
        "              break\n",
        "            axes1[j][k].set_axis_off()\n",
        "            axes1[j][k].imshow(x[i])\n",
        "            i += 1\n",
        "    plt.show()\n",
        "    return"
      ],
      "metadata": {
        "id": "D8CksA40fYtu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
      ],
      "metadata": {
        "id": "MaCOwRg2JcA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# construct a pca object from given data\n",
        "def get_pca(component, X):\n",
        "    pca = PCA(n_components=component)\n",
        "    pca.fit(X)\n",
        "    return pca\n",
        "\n",
        "def pca_encode(pca_model, X):\n",
        "  return pca_model.transform(X)\n",
        "\n",
        "def pca_decode(pca_model, X):\n",
        "  return pca_model.inverse_transform(X)"
      ],
      "metadata": {
        "id": "mZoNlVafgFPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# generate a kmeans model and label of input data\n",
        "def generate_kmeans_model(X, k):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    transformed = kmeans.fit_predict(X)\n",
        "    return kmeans, transformed\n",
        "\n",
        "# predict cluster labels of given data\n",
        "def kmeans_clustering(model, X):\n",
        "    return model.predict(X)\n",
        "\n",
        "# transform a list of labels respect to index into a dictionary of {cluster)num:[index]}\n",
        "def clusters_to_index(cluster_labels):\n",
        "    dict = {}\n",
        "    for i in range(len(cluster_labels)):\n",
        "        if cluster_labels[i] in dict:\n",
        "            dict[cluster_labels[i]].append(i)\n",
        "        else:\n",
        "            dict[cluster_labels[i]] = [i]\n",
        "    return dict"
      ],
      "metadata": {
        "id": "JzHHb4ArgjL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_kmeans_pipeline(X, feature_vector_size, cluster_num):\n",
        "  pca_model = get_pca(feature_vector_size, X)\n",
        "  encoded_image = pca_encode(pca_model, X)\n",
        "  kmeans_model, data_index_cluster_labels = generate_kmeans_model(encoded_image, cluster_num)\n",
        "  cluster_index_dict = clusters_to_index(data_index_cluster_labels)\n",
        "  return pca_model, kmeans_model, cluster_index_dict"
      ],
      "metadata": {
        "id": "d2LlPLSDhv8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing kmeans cluster centers\n",
        "def visualize_kmeans_centre(kmeans_model, pca_model, row, col, colored = True):\n",
        "  cluster_centers = kmeans_model.cluster_centers_\n",
        "  cluster_centers_decoded = pca_decode(pca_model, cluster_centers)\n",
        "  cluster_centers_decoded_image = construct_image_from_flattern(cluster_centers_decoded, colored)\n",
        "  visualize_data(row, col, 15, cluster_centers_decoded_image)\n"
      ],
      "metadata": {
        "id": "AlZbj1ObilB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "def generate_GMM_model(X, components):\n",
        "  GMM_model = GaussianMixture(n_components = components)\n",
        "  transformed = GMM_model.fit_predict(X)\n",
        "  return GMM_model, transformed\n",
        "\n",
        "def GMM_clustering(model, X):\n",
        "  return model.predict(X)"
      ],
      "metadata": {
        "id": "G62bNdUQXPGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_GMM_pipeline(X, feature_vector_size, cluster_num):\n",
        "  pca_model = get_pca(feature_vector_size, X)\n",
        "  encoded_image = pca_encode(pca_model, X)\n",
        "  GMM_model, data_index_cluster_labels = generate_GMM_model(encoded_image, cluster_num)\n",
        "  cluster_index_dict = clusters_to_index(data_index_cluster_labels)\n",
        "  return pca_model, GMM_model, cluster_index_dict"
      ],
      "metadata": {
        "id": "TJ3mk4hNYANP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_GMM_means(GMM_model, pca_model, row, col, colored = True):\n",
        "  cluster_centers = GMM_model.means_\n",
        "  cluster_centers_decoded = pca_decode(pca_model, cluster_centers)\n",
        "  cluster_centers_decoded_image = construct_image_from_flattern(cluster_centers_decoded, colored)\n",
        "  visualize_data(row, col, 15, cluster_centers_decoded_image)"
      ],
      "metadata": {
        "id": "KQR0MczoYkDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def bench_k_means(kmeans, name, data, labels):\n",
        "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    kmeans : KMeans instance\n",
        "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
        "        already set.\n",
        "    name : str\n",
        "        Name given to the strategy. It will be used to show the results in a\n",
        "        table.\n",
        "    data : ndarray of shape (n_samples, n_features)\n",
        "        The data to cluster.\n",
        "    labels : ndarray of shape (n_samples,)\n",
        "        The labels used to compute the clustering metrics which requires some\n",
        "        supervision.\n",
        "    \"\"\"\n",
        "    t0 = time()\n",
        "    #estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
        "    kmeans_model = kmeans.fit(data)\n",
        "    fit_time = time() - t0\n",
        "    results = [name, fit_time, kmeans_model.inertia_]\n",
        "\n",
        "    # Define the metrics which require only the true labels and estimator\n",
        "    # labels\n",
        "    clustering_metrics = [\n",
        "        metrics.homogeneity_score,\n",
        "        metrics.completeness_score,\n",
        "        metrics.v_measure_score,\n",
        "        metrics.adjusted_rand_score,\n",
        "        metrics.normalized_mutual_info_score,\n",
        "    ]\n",
        "    results += [m(labels, kmeans_model.labels_) for m in clustering_metrics]\n",
        "\n",
        "    # The silhouette score requires the full dataset\n",
        "    results += [\n",
        "        metrics.silhouette_score(\n",
        "            data,\n",
        "            kmeans_model.labels_,\n",
        "            metric=\"euclidean\",\n",
        "            #sample_size=300,\n",
        "        ),\n",
        "        metrics.calinski_harabasz_score(data, kmeans_model.labels_),\n",
        "        metrics.davies_bouldin_score(data, kmeans_model.labels_)\n",
        "    ]\n",
        "\n",
        "    # Show the results\n",
        "    formatter_result = (\n",
        "        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\\t{:.3f}\\t{:.3f}\"\n",
        "    )\n",
        "    print(formatter_result.format(*results))\n",
        "\n",
        "    return kmeans_model"
      ],
      "metadata": {
        "id": "NDmlq8UMNXPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}